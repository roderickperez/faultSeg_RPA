{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb870a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math # For radians in rose plot\n",
    "from matplotlib.colors import ListedColormap\n",
    "import plotly.graph_objects as go\n",
    "import random\n",
    "import subprocess\n",
    "import json # Import json to load the stats file\n",
    "import pandas as pd # Import pandas for DataFrame summary\n",
    "from utilities import plot_fault_counts, plot_histogram, plot_rose_diagram, append_param_to_cmd, normalize, plot_fault_points, count_pixels   \n",
    "\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94db5971",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94db5971",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b793181",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40144b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 0) User picks parameters ────────────────────────────────────────────\n",
    "mask_mode    = 1 # 0 binary (0, 1) | 1 Multiclass (0, 1, 2), normal and inverse faults\n",
    "num_pairs    = 10\n",
    "cube_size    = 128\n",
    "ricker_freq = (5, 35)      # Hz or (min, max)\n",
    "seismic_noise = (0.1, 0.5)      # None | float | (min, max)\n",
    "ricker_dt    = 0.002\n",
    "wavelet_len  = 0.2        # s\n",
    "num_gauss    = (2,10)     # int or (min, max)\n",
    "num_faults = (0,4)       # int or (min, max) faults per cube\n",
    "max_disp   = (1, 100)   # int or (min, max) max displacement magnitude per cube\n",
    "strike     = (0,360)      # deg or (min, max) strike angle per fault\n",
    "dip        = (35,80) # deg or (min, max) dip angle per fault\n",
    "output_format = \"npy\" # npy, npz, or dat\n",
    "train_split = 0.7 # 70% for training\n",
    "val_split = 0.15 # 15% for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe2d2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────\n",
    "# 1)  ROOT directory – works in .py or notebook\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "try:                                   # for plain-.py execution\n",
    "    ROOT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:                      # Jupyter / IPython\n",
    "    ROOT_DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fe70c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────\n",
    "# 2)  Centralised output locations  (⇦ NEW)\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "DATA_DIR   = os.path.join(ROOT_DIR, \"generateSynthData\", \"data\")\n",
    "STATS_DIR  = os.path.join(ROOT_DIR, \"generateSynthData\", \"statistics\")\n",
    "IMAGE_DIR  = os.path.join(ROOT_DIR, \"generateSynthData\", \"images\")\n",
    "\n",
    "# make sure they exist\n",
    "for _d in (DATA_DIR, STATS_DIR, IMAGE_DIR):\n",
    "    os.makedirs(_d, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e85e450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 1) Call your script (passing parameters from notebook variables) ─────────────────\n",
    "script = os.path.join(ROOT_DIR, \"generateSynthData\", \"synthDataGeneration.py\")\n",
    "base_out = DATA_DIR # os.path.expanduser(\"/home/roderickperez/DS_PROJECTS/faultSeg/faultSeg_Wu_2019_Keras/new_data\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(base_out, exist_ok=True)\n",
    "\n",
    "print(f\"Running script: {script}\")\n",
    "print(f\"Output directory: {base_out}\")\n",
    "\n",
    "# Calculate the number of samples for each set\n",
    "num_train = int(np.floor(num_pairs * train_split))\n",
    "num_val = num_pairs - num_train\n",
    "\n",
    "# Construct the command\n",
    "cmd = [\n",
    "    \"python\", script,\n",
    "    \"--num-pairs\",     str(num_pairs),\n",
    "    \"--size\",          str(cube_size),\n",
    "    \"--dt\",            str(ricker_dt),\n",
    "    \"--length\",        str(wavelet_len),\n",
    "    \"--mask-mode\",     str(mask_mode),\n",
    "    \"--format\",        output_format,\n",
    "    \"--output-dir\",    base_out,\n",
    "    \"--train-split\",   str(train_split),\n",
    "    \"--val-split\",     str(val_split),\n",
    "    \"--faults\",        f\"{num_faults[0]},{num_faults[1]}\",\n",
    "    \"--max-disp\",      f\"{max_disp[0]},{max_disp[1]}\",\n",
    "    \"--strike\",        f\"{strike[0]},{strike[1]}\",\n",
    "    \"--dip\",           f\"{dip[0]},{dip[1]}\",\n",
    "    \"--freq\",          f\"{ricker_freq[0]},{ricker_freq[1]}\",\n",
    "    \"--num-gaussians\", f\"{num_gauss[0]},{num_gauss[1]}\",\n",
    "    \"--noise\",         f\"{seismic_noise[0]},{seismic_noise[1]}\",\n",
    "]\n",
    "\n",
    "# Run the command\n",
    "result = subprocess.run(cmd, text=True) #result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"Data generation successful!\")\n",
    "    print(result.stdout)\n",
    "else:\n",
    "    print(\"Data generation failed.\")\n",
    "    print(result.stderr)\n",
    "\n",
    "for fname in (\"statistics_full.json\",\n",
    "              \"statistics_train.json\",\n",
    "              \"statistics_validation.json\"):\n",
    "    src = os.path.join(base_out, fname)\n",
    "    dst = os.path.join(STATS_DIR, fname)\n",
    "    if os.path.exists(src):\n",
    "        try:\n",
    "            os.replace(src, dst)          # move (overwrite if exists)\n",
    "            print(f\"Moved {fname} → {STATS_DIR}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not move {fname}: {e}\")\n",
    "    else:\n",
    "        print(f\"Warning: {fname} not found in {base_out}\")\n",
    "        \n",
    "# Data generation statistics\n",
    "stats = {\n",
    "    \"total_pairs\": num_pairs,\n",
    "    \"train_pairs\": num_train,\n",
    "    \"validation_pairs\": num_val,\n",
    "}\n",
    "\n",
    "# Save statistics to a JSON file\n",
    "stats_file_path = os.path.join(STATS_DIR, \"stats_data.json\")\n",
    "with open(stats_file_path, 'w') as f:\n",
    "    json.dump(stats, f, indent=4)\n",
    "\n",
    "print(f\"Data generation complete. Statistics saved to {stats_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185bc1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 2) Check the output ────────────────────────────────────────────────\n",
    "# Check the total number of files created across all splits\n",
    "total_files_generated = 0\n",
    "for split in ['train', 'validation']:\n",
    "    seis_dir = os.path.join(base_out, split, 'seis')\n",
    "    if os.path.exists(seis_dir):\n",
    "        total_files_generated += len(os.listdir(seis_dir))\n",
    "\n",
    "if total_files_generated != num_pairs:\n",
    "    print(f\"\\nWarning: Mismatch in generated files ({total_files_generated}) and requested pairs ({num_pairs}). Check script output.\")\n",
    "else:\n",
    "    print(f\"\\nSuccessfully generated {total_files_generated} file pairs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b502e585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append parameters that can be fixed values or ranges\n",
    "def append_param_to_cmd(cmd_list, arg_name, value):\n",
    "    if value is not None:\n",
    "        if isinstance(value, tuple):\n",
    "            cmd_list += [f\"--{arg_name}\", f\"{value[0]},{value[1]}\"]\n",
    "        else:\n",
    "            cmd_list += [f\"--{arg_name}\", str(value)]\n",
    "\n",
    "append_param_to_cmd(cmd, \"freq\", ricker_freq)\n",
    "append_param_to_cmd(cmd, \"num-gaussians\", num_gauss)\n",
    "append_param_to_cmd(cmd, \"noise\", seismic_noise)\n",
    "append_param_to_cmd(cmd, \"faults\", num_faults)\n",
    "append_param_to_cmd(cmd, \"max-disp\", max_disp)\n",
    "append_param_to_cmd(cmd, \"strike\", strike)\n",
    "append_param_to_cmd(cmd, \"dip\", dip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050d26d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCommand to run:\")\n",
    "print(\" \".join(cmd)) # Print the corrected command\n",
    "print(\"\\nRunning subprocess...\")\n",
    "\n",
    "# run and capture output\n",
    "# check=False allows inspection of returncode without raising exception immediately\n",
    "result = subprocess.run(cmd, capture_output=True, text=True, check=False)\n",
    "print(\"=== STDOUT ===\\n\", result.stdout)\n",
    "print(\"=== STDERR ===\\n\", result.stderr)\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(f\"Error: Script exited with return code {result.returncode}\")\n",
    "    # Optionally print more context or raise error\n",
    "    # raise subprocess.CalledProcessError(result.returncode, cmd, output=result.stdout, stderr=result.stderr)\n",
    "    # Exit this cell or notebook execution if script failed critically?\n",
    "    # For now, just print error and continue, assuming stats file might still be partially saved\n",
    "else:\n",
    "    print(\"Script finished successfully.\")\n",
    "\n",
    "# Check if the expected number of files were generated\n",
    "total_files_generated = 0\n",
    "for split in ['train', 'validation']:\n",
    "    seis_dir = os.path.join(base_out, split, 'seis')\n",
    "    if os.path.exists(seis_dir):\n",
    "        total_files_generated += len([f for f in os.listdir(seis_dir) if f.endswith((\".npy\", \".npz\", \".dat\"))])\n",
    "\n",
    "if total_files_generated != num_pairs:\n",
    "    print(f\"\\nWarning: Mismatch in generated files ({total_files_generated}) and requested pairs ({num_pairs}). Check script output.\")\n",
    "else:\n",
    "    print(f\"\\nSuccessfully generated {total_files_generated} file pairs across all splits.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b287652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 2) Load generated data and statistics ───────────────────────────────────────────────────────\n",
    "\n",
    "# Define the base output directory\n",
    "#base_out = os.path.expanduser(\"/Users/roderickperez/Documents/DS_Projects/faultSegm/faultSeg_Wu_2019_Keras/output\")\n",
    "\n",
    "# Define the splits and their expected number of files\n",
    "splits = {\n",
    "    \"train\": num_train,\n",
    "    \"validation\": num_val\n",
    "}\n",
    "\n",
    "# Check if the expected number of files were generated for each split\n",
    "for split, expected_files in splits.items():\n",
    "    seis_dir = os.path.join(base_out, split, \"seis\")\n",
    "    fault_dir = os.path.join(base_out, split, \"fault\")\n",
    "\n",
    "    if os.path.exists(seis_dir):\n",
    "        files = sorted(f for f in os.listdir(seis_dir) if f.endswith((\".npy\", \".npz\", \".dat\")))\n",
    "        n_files = len(files)\n",
    "        if n_files != expected_files:\n",
    "            print(f\"Warning: Mismatch in generated files for '{split}' split. Expected {expected_files}, found {n_files} in {seis_dir}\")\n",
    "    else:\n",
    "        print(f\"Warning: Seismic output directory not found for '{split}' split: {seis_dir}\")\n",
    "\n",
    "    if not os.path.exists(fault_dir):\n",
    "        print(f\"Warning: Fault output directory not found for '{split}' split: {fault_dir}\")\n",
    "\n",
    "# Load the statistics files\n",
    "stats_data = {}\n",
    "for split in splits.keys():\n",
    "    stats_file_path = os.path.join(STATS_DIR, f\"statistics_{split}.json\")\n",
    "    if os.path.exists(stats_file_path):\n",
    "        with open(stats_file_path, 'r') as f:\n",
    "            stats_data[split] = json.load(f)\n",
    "            print(f\"Successfully loaded statistics for '{split}' split.\")\n",
    "    else:\n",
    "        print(f\"Warning: Statistics file not found for '{split}' split: {stats_file_path}\")\n",
    "\n",
    "# Load the full statistics file\n",
    "full_stats_path = os.path.join(STATS_DIR, \"statistics_full.json\")\n",
    "if os.path.exists(full_stats_path):\n",
    "    with open(full_stats_path, 'r') as f:\n",
    "        full_stats = json.load(f)\n",
    "        print(\"Successfully loaded full statistics.\")\n",
    "else:\n",
    "    print(f\"Warning: Full statistics file not found: {full_stats_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c923286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all statistics: full, prediction, train, and validation\n",
    "stats_files = {\n",
    "    \"full\": os.path.join(STATS_DIR, \"statistics_full.json\"),\n",
    "    \"train\": os.path.join(STATS_DIR, \"statistics_train.json\"),\n",
    "    \"validation\": os.path.join(STATS_DIR, \"statistics_validation.json\")\n",
    "}\n",
    "\n",
    "all_stats_data = {}\n",
    "for split, path in stats_files.items():\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            with open(path, 'r') as f:\n",
    "                all_stats_data[split] = json.load(f)\n",
    "            print(f\"Loaded statistics for '{split}' from {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading statistics for '{split}': {e}\")\n",
    "            all_stats_data[split] = None\n",
    "    else:\n",
    "        print(f\"Statistics file not found for '{split}': {path}\")\n",
    "        all_stats_data[split] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5c8142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 3) Display Statistics and Plots for All Splits Individually ──────────────────────────────\n",
    "\n",
    "# palette for stacked plots\n",
    "normal_colour  = (0.50, 0.00, 0.50)   # purple  (Normal)\n",
    "inverse_colour = (0.00, 0.80, 0.00)   # green   (Inverse)\n",
    "    \n",
    "for split_name in ['full', 'train', 'validation']:\n",
    "    stats = all_stats_data.get(split_name)\n",
    "    print(f\"\\n--- Displaying Statistics and Plots for {split_name} data ---\")\n",
    "\n",
    "    if not stats or 'cube_level_params' not in stats or 'all_fault_params' not in stats:\n",
    "        print(f\"No statistics available for '{split_name}' split.\")\n",
    "        continue\n",
    "\n",
    "    # Extract parameters for the current split\n",
    "    cube_level_params = stats['cube_level_params']\n",
    "    all_fault_params  = stats['all_fault_params']\n",
    "\n",
    "    # Create a summary DataFrame\n",
    "    df = pd.DataFrame(cube_level_params)\n",
    "    print(\"\\nCube Generation Parameter Summary:\")\n",
    "    print(df.describe())\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # NEW ─ gather all mask cubes for pixel-class statistics\n",
    "    # ------------------------------------------------------------------\n",
    "    mask_cubes = []\n",
    "    splits_to_scan = ['train', 'validation'] if split_name == 'full' else [split_name]\n",
    "    for split_dir in splits_to_scan:\n",
    "        mdir = os.path.join(base_out, split_dir, \"fault\")\n",
    "        if not os.path.isdir(mdir):\n",
    "            continue\n",
    "        for fname in sorted(os.listdir(mdir)):\n",
    "            if not fname.endswith((\".npy\", \".dat\")):\n",
    "                continue\n",
    "            fpath = os.path.join(mdir, fname)\n",
    "            if fname.endswith(\".npy\"):\n",
    "                mask_cubes.append(np.load(fpath))\n",
    "            else:  # .dat\n",
    "                mask_cubes.append(np.fromfile(fpath, dtype=np.uint8)\n",
    "                                  .reshape((cube_size,)*3))\n",
    "\n",
    "    overall_pct, mean_pct = count_pixels(mask_cubes, mask_mode)\n",
    "\n",
    "    print(\"\\nPixel-class distribution – OVERALL (% of all voxels):\")\n",
    "    for k, v in overall_pct.items():\n",
    "        print(f\"  {k:>10}: {v:6.2f} %\")\n",
    "    print(\"Pixel-class distribution – MEAN PER CUBE:\")\n",
    "    for k, v in mean_pct.items():\n",
    "        print(f\"  {k:>10}: {v:6.2f} %\")\n",
    "\n",
    "    # --- Generate and Display Plots ---\n",
    "    if not all_fault_params:\n",
    "        print(\"\\nNo fault parameters to plot for this split.\")\n",
    "        continue\n",
    "\n",
    "    # Extract fault-specific data for plotting\n",
    "    strikes               = [f['strike'] for f in all_fault_params]\n",
    "    dips                  = [f['dip'] for f in all_fault_params]\n",
    "    displacements         = [f['applied_disp_signed']     for f in all_fault_params]\n",
    "    vertical_displacements= [f['vertical_disp_component'] for f in all_fault_params]\n",
    "    noise_sigmas          = [p.get('noise_sigma')         for p in cube_level_params]\n",
    "\n",
    "    # Create a figure with a grid of subplots  (NOW 4×2)\n",
    "    fig = plt.figure(figsize=(20, 22))\n",
    "    fig.suptitle(f\"{split_name.capitalize()} Dataset Statistics\", fontsize=26)\n",
    "    gs = fig.add_gridspec(4, 2, height_ratios=[1, 1, 1, 0.7])\n",
    "\n",
    "    # Plot Fault Counts\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "\n",
    "    if mask_mode == 0:\n",
    "        # binary → one bar\n",
    "        ax1.bar(['Total'],\n",
    "                [len(all_fault_params)],\n",
    "                color=['skyblue'])\n",
    "    else:\n",
    "        # multiclass → two bars (Normal, Inverse)\n",
    "        normal_count  = sum(1 for f in all_fault_params if f['fault_type'] == 'Normal')\n",
    "        inverse_count = sum(1 for f in all_fault_params if f['fault_type'] == 'Inverse')\n",
    "        ax1.bar(['Normal', 'Inverse'],\n",
    "                [normal_count, inverse_count],\n",
    "                color=[normal_colour, inverse_colour])\n",
    "\n",
    "    ax1.set_title('Fault Type Distribution')\n",
    "    ax1.set_ylabel('Number of Faults')\n",
    "\n",
    "    # Plot Strike Rose Diagram\n",
    "    ax2 = fig.add_subplot(gs[0, 1], projection='polar')\n",
    "    if strikes:\n",
    "        if mask_mode == 1:\n",
    "            # --- separate angles\n",
    "            strikes_normal  = np.radians([f['strike'] for f in all_fault_params if f['fault_type'] == 'Normal'])\n",
    "            strikes_inverse = np.radians([f['strike'] for f in all_fault_params if f['fault_type'] == 'Inverse'])\n",
    "\n",
    "            num_bins = 18\n",
    "            bins_rad = np.linspace(0, 2*np.pi, num_bins + 1)\n",
    "            cnt_norm , _ = np.histogram(strikes_normal , bins=bins_rad)\n",
    "            cnt_inv  , _ = np.histogram(strikes_inverse, bins=bins_rad)\n",
    "\n",
    "            centres = bins_rad[:-1] + np.diff(bins_rad)/2\n",
    "            width   = np.diff(bins_rad)[0]\n",
    "\n",
    "            # first ring = Normal\n",
    "            ax2.bar(centres, cnt_norm,\n",
    "                    width=width,\n",
    "                    bottom=0.0,\n",
    "                    color=normal_colour,\n",
    "                    edgecolor='black',\n",
    "                    alpha=0.8,\n",
    "                    label='Normal')\n",
    "            # stacked on top = Inverse\n",
    "            ax2.bar(centres, cnt_inv,\n",
    "                    width=width,\n",
    "                    bottom=cnt_norm,\n",
    "                    color=inverse_colour,\n",
    "                    edgecolor='black',\n",
    "                    alpha=0.8,\n",
    "                    label='Inverse')\n",
    "\n",
    "            ax2.legend(loc='lower left', bbox_to_anchor=(1.05, 0.0))\n",
    "            ax2.set_title('Fault Strike Angles', va='bottom', y=1.1)\n",
    "        else:\n",
    "            plot_rose_diagram(strikes, ax=ax2, title='Fault Strike Angles')\n",
    "\n",
    "    # Plot Dip Histogram\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "\n",
    "    if dips:\n",
    "        if mask_mode == 1:\n",
    "            dips_normal  = [f['dip'] for f in all_fault_params if f['fault_type'] == 'Normal']\n",
    "            dips_inverse = [f['dip'] for f in all_fault_params if f['fault_type'] == 'Inverse']\n",
    "            ax3.hist([dips_normal, dips_inverse],\n",
    "                     bins='auto',\n",
    "                     stacked=True,\n",
    "                     color=[normal_colour, inverse_colour],\n",
    "                     label=['Normal', 'Inverse'],\n",
    "                     edgecolor='black')\n",
    "            ax3.legend()\n",
    "        else:\n",
    "            plot_histogram(dips, ax=ax3, title='Fault Dip Angles',\n",
    "                           xlabel='Dip (degrees)')\n",
    "        ax3.set_title('Fault Dip Angles')\n",
    "        ax3.set_xlabel('Dip (degrees)')\n",
    "\n",
    "    # Plot Displacement Histogram\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    if displacements:\n",
    "        if mask_mode == 1:\n",
    "            disp_normal  = [f['applied_disp_signed'] for f in all_fault_params if f['fault_type'] == 'Normal']\n",
    "            disp_inverse = [f['applied_disp_signed'] for f in all_fault_params if f['fault_type'] == 'Inverse']\n",
    "            ax4.hist([disp_normal, disp_inverse],\n",
    "                     bins='auto',\n",
    "                     stacked=True,\n",
    "                     color=[normal_colour, inverse_colour],\n",
    "                     label=['Normal', 'Inverse'],\n",
    "                     edgecolor='black')\n",
    "            ax4.legend()\n",
    "        else:\n",
    "            plot_histogram(displacements, ax=ax4, title='Applied Displacement',\n",
    "                           xlabel='Displacement (grid units)')\n",
    "        ax4.set_title('Applied Displacement')\n",
    "        ax4.set_xlabel('Displacement (grid units)')\n",
    "\n",
    "    # Plot Vertical Displacement Histogram\n",
    "    ax5 = fig.add_subplot(gs[2, 0])\n",
    "    if vertical_displacements:\n",
    "        if mask_mode == 1:\n",
    "            vdisp_normal  = [f['vertical_disp_component'] for f in all_fault_params if f['fault_type'] == 'Normal']\n",
    "            vdisp_inverse = [f['vertical_disp_component'] for f in all_fault_params if f['fault_type'] == 'Inverse']\n",
    "            ax5.hist([vdisp_normal, vdisp_inverse],\n",
    "                     bins='auto',\n",
    "                     stacked=True,\n",
    "                     color=[normal_colour, inverse_colour],\n",
    "                     label=['Normal', 'Inverse'],\n",
    "                     edgecolor='black')\n",
    "            ax5.legend()\n",
    "        else:\n",
    "            plot_histogram(vertical_displacements, ax=ax5,\n",
    "                           title='Vertical Displacement Component',\n",
    "                           xlabel='Vertical Displacement (grid units)')\n",
    "        ax5.set_title('Vertical Displacement Component')\n",
    "        ax5.set_xlabel('Vertical Displacement (grid units)')\n",
    "\n",
    "    # Plot Noise Sigma Histogram\n",
    "    ax6 = fig.add_subplot(gs[2, 1])\n",
    "    if any(n is not None for n in noise_sigmas):\n",
    "        plot_histogram([n for n in noise_sigmas if n is not None],\n",
    "                       ax=ax6, title='Seismic Noise Sigma',\n",
    "                       xlabel='Noise Sigma')\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # NEW subplot: pixel-class distribution\n",
    "    # ------------------------------------------------------------------\n",
    "    ax7 = fig.add_subplot(gs[3, :])\n",
    "    categories = list(overall_pct.keys())\n",
    "\n",
    "    if mask_mode == 0:\n",
    "        colours = ['dimgrey', 'skyblue'][:len(categories)]\n",
    "    else:\n",
    "        colour_map = {\n",
    "            'no_fault': 'dimgrey',\n",
    "            'normal'  : (0.50, 0.00, 0.50),  # purple\n",
    "            'inverse' : (0.00, 0.80, 0.00)   # green\n",
    "        }\n",
    "        colours = [colour_map[c] for c in categories]\n",
    "\n",
    "    ax7.bar(categories,\n",
    "            [overall_pct[k] for k in categories],\n",
    "            color=colours)\n",
    "    ax7.set_ylim(0, 100)\n",
    "    ax7.set_title('Pixel-Class Distribution (% of all voxels)')\n",
    "    ax7.set_ylabel('Percentage')\n",
    "    for i, k in enumerate(categories):\n",
    "        ax7.text(i, overall_pct[k] + 1, f\"{overall_pct[k]:.1f}%\", ha='center')\n",
    "\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    img_path = os.path.join(IMAGE_DIR, f\"{split_name}_dataset_stats.png\")\n",
    "    fig.savefig(img_path, dpi=300)\n",
    "    print(f\"Saved figure → {img_path}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c13b8c5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a31794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare colormaps for sample slices\n",
    "bw_cmap             = \"gray\"\n",
    "overlay_bw_cmap     = ListedColormap([(0,0,0,0), (1,1,1,1)]) # Transparent to White\n",
    "\n",
    "mask_color_alone_cmap   = ListedColormap([\n",
    "    (0.00, 0.00, 0.00, 1.0),  # 0 → black\n",
    "    (0.50, 0.00, 0.50, 1.0),  # 1 → purple (Normal)\n",
    "    (0.00, 0.80, 0.00, 1.0)   # 2 → green (Inverse)\n",
    "])\n",
    "mask_color_overlay_cmap = ListedColormap([\n",
    "    (0,0,0,0),           # 0 → transparent\n",
    "    (0.50,0.00,0.50,1),  # 1 → purple (Normal)\n",
    "    (0.00,0.80,0.00,1)   # 2 → green (Inverse)\n",
    "])\n",
    "\n",
    "# Select colormap depending on mask_mode\n",
    "if mask_mode == 0:\n",
    "    mask_alone_cmap_slice   = bw_cmap\n",
    "    mask_overlay_cmap_slice = overlay_bw_cmap\n",
    "else:\n",
    "    mask_alone_cmap_slice   = mask_color_alone_cmap\n",
    "    mask_overlay_cmap_slice = mask_color_overlay_cmap\n",
    "\n",
    "# Transparency factor for overlays\n",
    "overlay_alpha = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ee7a9f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca7b996",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6135f6",
   "metadata": {},
   "source": [
    "### 3D Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b695fcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "# cube_size = 128  # or whatever you used\n",
    "# mask_mode = 1    # or 1\n",
    "#base_out = \"/Users/roderickperez/Documents/DS_Projects/faultSegm/faultSeg_Wu_2019_Keras/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1dba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize to 0-255 uint8\n",
    "def normalize(v):\n",
    "    return ((v - v.min()) / (v.max() - v.min()) * 255).astype(np.uint8)\n",
    "\n",
    "# Function to plot fault points\n",
    "def plot_fault_points(fig, slice_mask, axis_index, slice_type, color, label, nx, ny, nz):\n",
    "    if slice_type == \"inline\":\n",
    "        ks, js = np.where(slice_mask)\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=np.full_like(ks, axis_index),\n",
    "            y=js,\n",
    "            z=nz - 1 - ks,\n",
    "            mode='markers',\n",
    "            marker=dict(color=color, size=2),\n",
    "            name=label\n",
    "        ))\n",
    "    elif slice_type == \"crossline\":\n",
    "        ks, is_ = np.where(slice_mask)\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=is_,\n",
    "            y=np.full_like(is_, axis_index),\n",
    "            z=nz - 1 - ks,\n",
    "            mode='markers',\n",
    "            marker=dict(color=color, size=2),\n",
    "            name=label\n",
    "        ))\n",
    "    elif slice_type == \"timeslice\":\n",
    "        is_, js = np.where(slice_mask)\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=is_,\n",
    "            y=js,\n",
    "            z=np.full_like(is_, axis_index),\n",
    "            mode='markers',\n",
    "            marker=dict(color=color, size=2),\n",
    "            name=label\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c50aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "# cube_size = 128  # Adjust as needed\n",
    "# mask_mode = 1    # 0 = binary masks; 1 = multiclass\n",
    "#base_out = \"/Users/roderickperez/Documents/DS_Projects/faultSegm/faultSeg_Wu_2019_Keras/output\"\n",
    "\n",
    "# Normalization helper\n",
    "def normalize(v):\n",
    "    return ((v - v.min()) / (v.max() - v.min()) * 255).astype(np.uint8)\n",
    "\n",
    "# Fault plotting function\n",
    "def plot_fault_points(fig, slice_mask, axis_index, slice_type, color, label, nx, ny, nz):\n",
    "    if slice_type == \"inline\":\n",
    "        ks, js = np.where(slice_mask)\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=np.full_like(ks, axis_index),\n",
    "            y=js,\n",
    "            z=nz - 1 - ks,\n",
    "            mode='markers',\n",
    "            marker=dict(color=color, size=2, opacity=1),\n",
    "            name=label\n",
    "        ))\n",
    "    elif slice_type == \"crossline\":\n",
    "        ks, is_ = np.where(slice_mask)\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=is_,\n",
    "            y=np.full_like(is_, axis_index),\n",
    "            z=nz - 1 - ks,\n",
    "            mode='markers',\n",
    "            marker=dict(color=color, size=2, opacity=1),\n",
    "            name=label\n",
    "        ))\n",
    "    elif slice_type == \"timeslice\":\n",
    "        is_, js = np.where(slice_mask)\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=is_,\n",
    "            y=js,\n",
    "            z=np.full_like(is_, axis_index),\n",
    "            mode='markers',\n",
    "            marker=dict(color=color, size=2, opacity=1),\n",
    "            name=label\n",
    "        ))\n",
    "\n",
    "# Loop over splits\n",
    "for split in [\"train\", \"validation\"]:\n",
    "    seismic_dir = os.path.join(base_out, split, \"seis\")\n",
    "    mask_dir = os.path.join(base_out, split, \"fault\")\n",
    "\n",
    "    if not (os.path.exists(seismic_dir) and os.path.exists(mask_dir)):\n",
    "        print(f\"[{split}] Skipping: directories not found.\")\n",
    "        continue\n",
    "\n",
    "    files = sorted(set(os.listdir(seismic_dir)).intersection(os.listdir(mask_dir)))\n",
    "    if not files:\n",
    "        print(f\"[{split}] No files to visualize.\")\n",
    "        continue\n",
    "\n",
    "    selected_file = random.choice([f for f in files if f.endswith((\".npy\", \".dat\"))])\n",
    "    print(f\"[{split}] Selected file: {selected_file}\")\n",
    "\n",
    "    # Load data\n",
    "    seismic_path = os.path.join(seismic_dir, selected_file)\n",
    "    mask_path = os.path.join(mask_dir, selected_file)\n",
    "\n",
    "    if selected_file.endswith(\".npy\"):\n",
    "        seismic = np.load(seismic_path)\n",
    "        mask = np.load(mask_path)\n",
    "    else:\n",
    "        seismic = np.fromfile(seismic_path, dtype=np.float32).reshape((cube_size, cube_size, cube_size))\n",
    "        mask = np.fromfile(mask_path, dtype=np.uint8).reshape((cube_size, cube_size, cube_size))\n",
    "\n",
    "    nx, ny, nz = seismic.shape\n",
    "\n",
    "    # Slices\n",
    "    inline     = seismic[nx//2,:,:].T\n",
    "    crossline  = seismic[:,ny//2,:].T\n",
    "    timeslice  = seismic[:,:,nz//2]\n",
    "\n",
    "    inline_m   = mask[nx//2,:,:].T\n",
    "    cross_m    = mask[:,ny//2,:].T\n",
    "    timeslice_m= mask[:,:,nz//2]\n",
    "\n",
    "    inline_norm    = normalize(inline)\n",
    "    crossline_norm = normalize(crossline)\n",
    "    timeslice_norm = normalize(timeslice)\n",
    "\n",
    "    # Create 3D figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Seismic surfaces\n",
    "    fig.add_trace(go.Surface(\n",
    "        z=np.tile(np.arange(nz)[::-1], (ny, 1)).T,\n",
    "        x=np.full((nz, ny), nx//2),\n",
    "        y=np.tile(np.arange(ny), (nz, 1)),\n",
    "        surfacecolor=inline_norm,\n",
    "        colorscale=\"Gray\",\n",
    "        showscale=False,\n",
    "        opacity=1\n",
    "    ))\n",
    "    fig.add_trace(go.Surface(\n",
    "        z=np.tile(np.arange(nz)[::-1], (nx, 1)).T,\n",
    "        x=np.tile(np.arange(nx), (nz, 1)),\n",
    "        y=np.full((nz, nx), ny//2),\n",
    "        surfacecolor=crossline_norm,\n",
    "        colorscale=\"Gray\",\n",
    "        showscale=False,\n",
    "        opacity=1\n",
    "    ))\n",
    "    fig.add_trace(go.Surface(\n",
    "        z=np.full((ny, nx), nz//2),\n",
    "        x=np.tile(np.arange(nx), (ny, 1)),\n",
    "        y=np.tile(np.arange(ny).reshape(-1,1), (1, nx)),\n",
    "        surfacecolor=np.rot90(timeslice_norm, k=-1),\n",
    "        colorscale=\"Gray\",\n",
    "        showscale=False,\n",
    "        opacity=1\n",
    "    ))\n",
    "\n",
    "    # Fault overlays\n",
    "    if mask_mode == 0:\n",
    "        plot_fault_points(fig, inline_m==1, nx//2, \"inline\", \"white\", \"Inline Faults\", nx, ny, nz)\n",
    "        plot_fault_points(fig, cross_m==1, ny//2, \"crossline\", \"white\", \"Crossline Faults\", nx, ny, nz)\n",
    "        plot_fault_points(fig, timeslice_m==1, nz//2, \"timeslice\", \"white\", \"Time-Slice Faults\", nx, ny, nz)\n",
    "    elif mask_mode == 1:\n",
    "        plot_fault_points(fig, inline_m==1, nx//2, \"inline\", \"green\", \"Inline Normal Faults\", nx, ny, nz)\n",
    "        plot_fault_points(fig, inline_m==2, nx//2, \"inline\", \"purple\", \"Inline Inverse Faults\", nx, ny, nz)\n",
    "        plot_fault_points(fig, cross_m==1, ny//2, \"crossline\", \"green\", \"Crossline Normal Faults\", nx, ny, nz)\n",
    "        plot_fault_points(fig, cross_m==2, ny//2, \"crossline\", \"purple\", \"Crossline Inverse Faults\", nx, ny, nz)\n",
    "        plot_fault_points(fig, timeslice_m==1, nz//2, \"timeslice\", \"green\", \"Time-Slice Normal Faults\", nx, ny, nz)\n",
    "        plot_fault_points(fig, timeslice_m==2, nz//2, \"timeslice\", \"purple\", \"Time-Slice Inverse Faults\", nx, ny, nz)\n",
    "\n",
    "    # Layout\n",
    "    fig.update_layout(\n",
    "        title=f\"3D Fault Visualization — {split.upper()} / {selected_file}\",\n",
    "        scene=dict(\n",
    "            xaxis=dict(visible=False),\n",
    "            yaxis=dict(visible=False),\n",
    "            zaxis=dict(visible=False),\n",
    "            aspectmode=\"data\"\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, t=30, b=0)\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d277238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5486d681",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faultseg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
