{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a71f81e2",
   "metadata": {},
   "source": [
    "# U-Net Model Training for Fault Segmentation (keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10946dda",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a196097b",
   "metadata": {},
   "source": [
    "First, we import all the necessary libraries. This includes standard libraries like `os` and `numpy`, deep learning libraries from `tensorflow.keras`, and the custom modules `DataGenerator` and `unet` from our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a366f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# TensorFlow Keras imports\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard # type: ignore\n",
    "from tensorflow.keras.models import Model # type: ignore\n",
    "from tensorflow.keras.layers import Input, Conv3D, MaxPooling3D, UpSampling3D, concatenate # type: ignore\n",
    "from tensorflow.keras.optimizers import Adam # type: ignore\n",
    "from tensorflow.keras import backend as K # type: ignore\n",
    "from tensorflow.keras.utils import Sequence, get_custom_objects # type: ignore\n",
    "\n",
    "from unet3_keras import cross_entropy_balanced\n",
    "get_custom_objects()['cross_entropy_balanced'] = cross_entropy_balanced\n",
    "\n",
    "# Set random seeds for reproducibility, as in the original script\n",
    "np.random.seed(12345)\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaa52e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ROOT_DIR = os.path.dirname(os.path.abspath(__file__))  # for .py scripts\n",
    "except NameError:\n",
    "    ROOT_DIR = os.getcwd()  # for Jupyter notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24c5261",
   "metadata": {},
   "source": [
    "## Step 1a: Verify GPU Availability\n",
    "This cell checks if TensorFlow can detect a GPU. If a GPU is available, it enables memory growth to prevent TensorFlow from allocating all of the GPU's memory at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612dd6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        \n",
    "        # GPU Warm-up to ensure it's ready\n",
    "        print(\"Warming up GPU...\")\n",
    "        with tf.device('/GPU:0'):\n",
    "            dummy_tensor = tf.random.normal([1, 1])\n",
    "            _ = dummy_tensor * 2.0\n",
    "        print(\"GPU is warmed up and ready.\")\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU detected. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f643c6",
   "metadata": {},
   "source": [
    "## Step 2: Define Paths and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d7e6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Input: Original .dat directories ---\n",
    "base_dir = os.path.abspath(os.path.join(ROOT_DIR, \"..\", \"data\", \"data_from_Wu\"))\n",
    "train_seis_dir = os.path.join(base_dir, \"train/seis\")\n",
    "train_fault_dir = os.path.join(base_dir, \"train/fault\")\n",
    "validation_seis_dir = os.path.join(base_dir, \"validation/seis\")\n",
    "validation_fault_dir = os.path.join(base_dir, \"validation/fault\")\n",
    "prediction_seis_dir = os.path.join(base_dir, \"prediction/f3d\")\n",
    "\n",
    "# --- Output: Processed .npy directories ---\n",
    "processed_data_dir = os.path.join(ROOT_DIR, \"data\")\n",
    "train_dir_new = os.path.join(processed_data_dir, 'train_npy')\n",
    "validation_dir_new = os.path.join(processed_data_dir, 'validation_npy')\n",
    "prediction_dir_new = os.path.join(processed_data_dir, 'prediction_npy')\n",
    "\n",
    "# --- Model/Log dirs (unchanged) ---\n",
    "model_dir = os.path.join(ROOT_DIR, \"model\")\n",
    "log_dir = os.path.join(ROOT_DIR, \"output\", \"logs\")\n",
    "checkpoint_dir = os.path.join(ROOT_DIR, \"output\", \"checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3794726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Parameters (from train_keras.py) ---\n",
    "params = {\n",
    "    'batch_size': 1,\n",
    "    'dim': (128, 128, 128),\n",
    "    'n_channels': 1,\n",
    "    'shuffle': True\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce43d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "epochs = 25\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184dd295",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e67420",
   "metadata": {},
   "source": [
    "This step involves converting the original `.dat` files into the `.npy` format that our `DataGenerator` expects. We also split the data into training and validation sets. This is a crucial preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23dba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dat_to_npy():\n",
    "    \"\"\"Creates directories and converts .dat files to .npy format.\"\"\"\n",
    "\n",
    "    # Clean up old processed data folders\n",
    "    for folder in [train_dir_new, validation_dir_new, prediction_dir_new]:\n",
    "        if os.path.exists(folder):\n",
    "            shutil.rmtree(folder)\n",
    "            print(f\"Removed old directory: {folder}\")\n",
    "\n",
    "    # Create new directory structure\n",
    "    os.makedirs(os.path.join(train_dir_new, 'seis'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(train_dir_new, 'fault'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(validation_dir_new, 'seis'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(validation_dir_new, 'fault'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(prediction_dir_new, 'seis'), exist_ok=True)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(\"Created new directory structure for processed data and model artifacts.\")\n",
    "\n",
    "    def process_files(file_list, source_seis, source_fault, dest_dir):\n",
    "        \"\"\"Reads .dat, reshapes, and saves as .npy.\"\"\"\n",
    "        for filename in file_list:\n",
    "            # Process seismic data\n",
    "            seis_dat_path = os.path.join(source_seis, filename)\n",
    "            seis_npy_path = os.path.join(dest_dir, 'seis', filename.replace('.dat', '.npy'))\n",
    "            seismic_data = np.fromfile(seis_dat_path, dtype=np.single)\n",
    "\n",
    "            if seismic_data.size == np.prod(params['dim']):\n",
    "                # Normal case (1 volume)\n",
    "                seismic_data = seismic_data.reshape(params['dim'])\n",
    "                np.save(seis_npy_path, seismic_data)\n",
    "            elif seismic_data.size % np.prod(params['dim']) == 0:\n",
    "                # Special case: prediction files contain multiple volumes\n",
    "                num_volumes = seismic_data.size // np.prod(params['dim'])\n",
    "                seismic_data = seismic_data.reshape((num_volumes, *params['dim']))\n",
    "                for i in range(num_volumes):\n",
    "                    sub_volume = seismic_data[i]\n",
    "                    volume_name = filename.replace('.dat', f'_vol{i+1}.npy')\n",
    "                    np.save(os.path.join(dest_dir, 'seis', volume_name), sub_volume)\n",
    "                print(f\"üîÅ Split {filename} into {num_volumes} volumes.\")\n",
    "                continue\n",
    "            else:\n",
    "                raise ValueError(f\"‚ùå Unexpected shape for file {filename} with {seismic_data.size} elements.\")\n",
    "\n",
    "            # Process fault data if available\n",
    "            fault_dat_path = os.path.join(source_fault, filename)\n",
    "            fault_npy_path = os.path.join(dest_dir, 'fault', filename.replace('.dat', '.npy'))\n",
    "            if os.path.exists(fault_dat_path):\n",
    "                fault_data = np.fromfile(fault_dat_path, dtype=np.single).reshape(params['dim'])\n",
    "                np.save(fault_npy_path, fault_data)\n",
    "        print(f\"Processed {len(file_list)} files for {os.path.basename(dest_dir)}.\")\n",
    "\n",
    "    # Get file lists\n",
    "    train_dat_files = [f for f in os.listdir(train_seis_dir) if f.endswith('.dat')]\n",
    "    validation_dat_files = [f for f in os.listdir(validation_seis_dir) if f.endswith('.dat')]\n",
    "\n",
    "    if os.path.exists(prediction_seis_dir):\n",
    "        prediction_dat_files = [f for f in os.listdir(prediction_seis_dir) if f.endswith('.dat')]\n",
    "    else:\n",
    "        prediction_dat_files = []\n",
    "        print(f\"‚ö†Ô∏è Warning: Prediction folder '{prediction_seis_dir}' not found. Skipping prediction data.\")\n",
    "\n",
    "    print(\"\\nStarting data conversion...\")\n",
    "    process_files(train_dat_files, train_seis_dir, train_fault_dir, train_dir_new)\n",
    "    process_files(validation_dat_files, validation_seis_dir, validation_fault_dir, validation_dir_new)\n",
    "    process_files(prediction_dat_files, prediction_seis_dir, prediction_seis_dir, prediction_dir_new)\n",
    "    print(\"\\n‚úÖ Data preparation complete. All .dat files converted to .npy.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ee6ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the conversion\n",
    "convert_dat_to_npy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c4764f",
   "metadata": {},
   "source": [
    "## Step 4: Create Data Generators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86f0b67",
   "metadata": {},
   "source": [
    "Now that our data is in the correct format and location, we can use the `DataGenerator` from `utils.py` to feed it to our model efficiently. We create one generator for the training set and one for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5bb9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    'Generates data for a Keras model'\n",
    "    def __init__(self, dpath, fpath, data_IDs, batch_size, dim, n_channels, shuffle):\n",
    "        'Initialization'\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.dpath = dpath\n",
    "        self.fpath = fpath\n",
    "        self.batch_size = batch_size\n",
    "        self.data_IDs = data_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.data_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one batch of data'\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        data_IDs_temp = [self.data_IDs[k] for k in indexes]\n",
    "        X, Y = self.__data_generation(data_IDs_temp)\n",
    "        return X, Y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.data_IDs))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, data_IDs_temp):\n",
    "        'Generates data containing batch_size samples with augmentation'\n",
    "        # The augmentation doubles the batch size\n",
    "        effective_batch_size = len(data_IDs_temp) * 2\n",
    "        X = np.empty((effective_batch_size, *self.dim, self.n_channels), dtype=np.single)\n",
    "        Y = np.empty((effective_batch_size, *self.dim, self.n_channels), dtype=np.single)\n",
    "        \n",
    "        for i, ID in enumerate(data_IDs_temp):\n",
    "            # Load pre-converted .npy files\n",
    "            gx = np.load(os.path.join(self.dpath, ID + '.npy'))\n",
    "            fx = np.load(os.path.join(self.fpath, ID + '.npy'))\n",
    "            \n",
    "            # Normalize seismic data\n",
    "            xm = np.mean(gx)\n",
    "            xs = np.std(gx)\n",
    "            gx = (gx - xm) / (xs + 1e-8) # Add epsilon to avoid division by zero\n",
    "            \n",
    "            # Transpose as in original script\n",
    "            gx = np.transpose(gx)\n",
    "            fx = np.transpose(fx)\n",
    "\n",
    "            gx_reshaped = np.reshape(gx, (*self.dim, self.n_channels))\n",
    "            fx_reshaped = np.reshape(fx, (*self.dim, self.n_channels))\n",
    "            \n",
    "            # Original and flipped data for augmentation\n",
    "            X[i * 2, ] = gx_reshaped\n",
    "            Y[i * 2, ] = fx_reshaped\n",
    "            X[i * 2 + 1, ] = np.flipud(gx_reshaped)\n",
    "            Y[i * 2 + 1, ] = np.flipud(fx_reshaped)\n",
    "\n",
    "        return X, Y\n",
    "\n",
    "# --- Instantiate the Generators ---\n",
    "seismPathT = os.path.join(train_dir_new, \"seis/\")\n",
    "faultPathT = os.path.join(train_dir_new, \"fault/\")\n",
    "seismPathV = os.path.join(validation_dir_new, \"seis/\")\n",
    "faultPathV = os.path.join(validation_dir_new, \"fault/\")\n",
    "\n",
    "train_ID = [os.path.splitext(f)[0] for f in os.listdir(seismPathT)]\n",
    "valid_ID = [os.path.splitext(f)[0] for f in os.listdir(seismPathV)]\n",
    "\n",
    "train_generator = DataGenerator(dpath=seismPathT, fpath=faultPathT, data_IDs=train_ID, **params)\n",
    "valid_generator = DataGenerator(dpath=seismPathV, fpath=faultPathV, data_IDs=valid_ID, **params)\n",
    "\n",
    "print(\"Data generators for training and validation have been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06148647",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of training pairs: {len(train_ID)}\")\n",
    "print(f\"Number of validation pairs: {len(valid_ID)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0746cdb7",
   "metadata": {},
   "source": [
    "## Step 5: Build and Compile the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9493858",
   "metadata": {},
   "source": [
    "We will now use the `unet` function from `unet3.py` to build our model architecture. After building it, we compile it with an optimizer, a loss function, and metrics to monitor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df634b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet(input_size=(128, 128, 128, 1)):\n",
    "    \"\"\"Defines the 3D U-Net model architecture from unet3_keras.py.\"\"\"\n",
    "    inputs = Input(input_size)\n",
    "    \n",
    "    # Encoder Path\n",
    "    conv1 = Conv3D(16, (3, 3, 3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv3D(16, (3, 3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling3D(pool_size=(2, 2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv3D(32, (3, 3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv3D(32, (3, 3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling3D(pool_size=(2, 2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling3D(pool_size=(2, 2, 2))(conv3)\n",
    "\n",
    "    # Bottleneck\n",
    "    conv4 = Conv3D(128, (3, 3, 3), activation='relu', padding='same')(pool3)\n",
    "    conv4 = Conv3D(128, (3, 3, 3), activation='relu', padding='same')(conv4)\n",
    "\n",
    "    # Decoder Path\n",
    "    up5 = concatenate([UpSampling3D(size=(2, 2, 2))(conv4), conv3], axis=-1)\n",
    "    conv5 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(up5)\n",
    "    conv5 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(conv5)\n",
    "\n",
    "    up6 = concatenate([UpSampling3D(size=(2, 2, 2))(conv5), conv2], axis=-1)\n",
    "    conv6 = Conv3D(32, (3, 3, 3), activation='relu', padding='same')(up6)\n",
    "    conv6 = Conv3D(32, (3, 3, 3), activation='relu', padding='same')(conv6)\n",
    "\n",
    "    up7 = concatenate([UpSampling3D(size=(2, 2, 2))(conv6), conv1], axis=-1)\n",
    "    conv7 = Conv3D(16, (3, 3, 3), activation='relu', padding='same')(up7)\n",
    "    conv7 = Conv3D(16, (3, 3, 3), activation='relu', padding='same')(conv7)\n",
    "\n",
    "    # Output Layer\n",
    "    conv8 = Conv3D(1, (1, 1, 1), activation='sigmoid')(conv7)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[conv8])\n",
    "    return model\n",
    "\n",
    "def _to_tensor(x, dtype):\n",
    "    \"\"\"Convert the input `x` to a tensor of type `dtype`.\"\"\"\n",
    "    return tf.convert_to_tensor(x, dtype=dtype)\n",
    "\n",
    "def cross_entropy_balanced(y_true, y_pred):\n",
    "    \"\"\"Custom balanced cross-entropy loss function from unet3_keras.py.\"\"\"\n",
    "    _epsilon = _to_tensor(K.epsilon(), y_pred.dtype.base_dtype)\n",
    "    y_pred = tf.clip_by_value(y_pred, _epsilon, 1 - _epsilon)\n",
    "    y_pred = tf.math.log(y_pred / (1 - y_pred))\n",
    "\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    count_neg = tf.reduce_sum(1. - y_true)\n",
    "    count_pos = tf.reduce_sum(y_true)\n",
    "    beta = count_neg / (count_neg + count_pos)\n",
    "    pos_weight = beta / (1 - beta)\n",
    "\n",
    "    cost = tf.nn.weighted_cross_entropy_with_logits(logits=y_pred, labels=y_true, pos_weight=pos_weight)\n",
    "    cost = tf.reduce_mean(cost * (1 - beta))\n",
    "    \n",
    "    return tf.where(tf.equal(count_pos, 0.0), 0.0, cost)\n",
    "\n",
    "# Build and compile the model\n",
    "model = unet(input_size=(*params['dim'], params['n_channels']))\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate), \n",
    "              loss=cross_entropy_balanced,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368880bc",
   "metadata": {},
   "source": [
    "## Step 6: Define Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee6c39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(checkpoint_dir, \"fseg-{epoch:02d}.keras\") # <-- FIXED\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=False, mode='max')\n",
    "\n",
    "# TensorBoard callback for logging\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# List of callbacks to pass to the model\n",
    "callbacks_list = [checkpoint, tensorboard_callback]\n",
    "\n",
    "print(\"Callbacks defined. Checkpoints will be saved in the new .keras format.\")\n",
    "print(\"Starting model training...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c3ef8a",
   "metadata": {},
   "source": [
    "## Step 7: Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b633c17c",
   "metadata": {},
   "source": [
    "With everything set up, we can now start training the model using the `fit` method. The training and validation data will be provided through our generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87ca346",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_generator,\n",
    "                    validation_data=valid_generator,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks_list,\n",
    "                    verbose=1)\n",
    "\n",
    "print(\"\\nModel training has been completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de57e5d",
   "metadata": {},
   "source": [
    "## Step 8: Save the Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d6cf12",
   "metadata": {},
   "source": [
    "After training is complete, we save the final model to the `model` directory for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4b82b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique name for the final model using the .hdf5 extension\n",
    "now = datetime.now()\n",
    "date_time = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "num_pairs = len(train_ID)\n",
    "model_name = f\"unet_keras_model_{num_pairs}pairs_{epochs}epochs_{date_time}.keras\" # <-- FIXED\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "\n",
    "# Save the model\n",
    "model.save(model_path)\n",
    "\n",
    "print(f\"The final model has been saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249d59e8",
   "metadata": {},
   "source": [
    "## Step 9: Visualize Training History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900aee99",
   "metadata": {},
   "source": [
    "Finally, we visualize the training and validation accuracy and loss over the epochs. This helps us understand how well the model has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e9def3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_history(history, save_path=None):\n",
    "    \"\"\"\n",
    "    Plots the training and validation accuracy and loss from a Keras history object.\n",
    "    \"\"\"\n",
    "    print(\"Plotting training history...\")\n",
    "    \n",
    "    # Create a figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "    # --- Plot Accuracy ---\n",
    "    ax1.plot(history.history['accuracy'], label='Training Accuracy', color='b', marker='o', linestyle='-')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy', color='r', marker='o', linestyle='--')\n",
    "    ax1.set_title('Model Accuracy', fontsize=18)\n",
    "    ax1.set_ylabel('Accuracy', fontsize=14)\n",
    "    ax1.set_xlabel('Epoch', fontsize=14)\n",
    "    ax1.legend(loc='lower right', fontsize=12)\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # --- Plot Loss ---\n",
    "    ax2.plot(history.history['loss'], label='Training Loss', color='b', marker='o', linestyle='-')\n",
    "    ax2.plot(history.history['val_loss'], label='Validation Loss', color='r', marker='o', linestyle='--')\n",
    "    ax2.set_title('Model Loss', fontsize=18)\n",
    "    ax2.set_ylabel('Loss', fontsize=14)\n",
    "    ax2.set_xlabel('Epoch', fontsize=14)\n",
    "    ax2.legend(loc='upper right', fontsize=12)\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"History plot saved to: {save_path}\")\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdc4908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use os.path.splitext to robustly get the model name without its extension\n",
    "base_name, _ = os.path.splitext(model_name)\n",
    "plot_name = f\"{base_name}_history.png\"\n",
    "plot_path = os.path.join(ROOT_DIR, \"output\", \"history_plots\", plot_name)\n",
    "\n",
    "# Ensure history_plots directory exists\n",
    "os.makedirs(os.path.dirname(plot_path), exist_ok=True)\n",
    "\n",
    "# Call the function to save plot\n",
    "show_history(history, save_path=plot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af53163",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faultseg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
