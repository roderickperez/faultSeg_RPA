{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df7329f1",
   "metadata": {},
   "source": [
    "# Prediction with a pre‑trained model (TensorFlow)\n",
    "\n",
    "This notebook demonstrates how to use a pre‑trained **3D U‑Net** model to perform fault segmentation on new seismic data.  \n",
    "It covers two scenarios:\n",
    "\n",
    "1. **Simple prediction** – run inference on a single validation cube to verify the model.\n",
    "2. **Complex prediction** – run inference on a large F3 field volume using an overlap‑and‑blend tiling strategy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed7870a",
   "metadata": {},
   "source": [
    "## Step&nbsp;1 · Import libraries and configure environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee4f8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorflow.keras.models import load_model\n",
    "# --- custom U‑Net + loss ---\n",
    "from unet3_tf import cross_entropy_balanced   # unet3_tf.py must expose it\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# GPU configuration (identical to training notebook)\n",
    "# ------------------------------------------------------------------\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(len(gpus), \"Physical GPUs,\", len(tf.config.list_logical_devices('GPU')), \"Logical GPUs\")\n",
    "    except RuntimeError as exc:\n",
    "        print(exc)\n",
    "else:\n",
    "    print(\"No GPU detected – running on CPU.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Robust project‑root detection (works in notebook *and* .py script)\n",
    "# ------------------------------------------------------------------\n",
    "try:\n",
    "    ROOT_DIR = os.path.dirname(os.path.abspath(__file__))   # running as script\n",
    "except NameError:\n",
    "    ROOT_DIR = os.getcwd()                                  # running in notebook\n",
    "print(\"ROOT_DIR =\", ROOT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed78a990",
   "metadata": {},
   "source": [
    "## Step&nbsp;2 · Define paths and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93a9f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Directory tree — mirrors the training notebook ─────────────────\n",
    "base_dir            = os.path.abspath(os.path.join(ROOT_DIR, \"..\", \"data\", \"data_from_Wu\"))\n",
    "processed_data_dir  = os.path.join(ROOT_DIR, \"data\")                    # npy cubes\n",
    "validation_dir_new  = os.path.join(processed_data_dir, \"validation_npy\")\n",
    "prediction_dir_f3d  = os.path.join(base_dir, \"prediction\", \"f3d\")\n",
    "model_dir           = os.path.join(ROOT_DIR, \"model\")\n",
    "\n",
    "# ── Model file (edit if you switch models) ───────────────────────────\n",
    "model_name = \"unet_tf_model_200pairs_10epochs_2025-07-29_18-18-55.keras\"\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "\n",
    "# ── Model input size (must match training) ───────────────────────────\n",
    "patch_n1 = patch_n2 = patch_n3 = 128\n",
    "\n",
    "print(\"Model           :\", model_path)\n",
    "print(\"Validation cubes:\", validation_dir_new)\n",
    "print(\"F3 field folder :\", prediction_dir_f3d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5814c0cf",
   "metadata": {},
   "source": [
    "## Step 3 · Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca1f45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "\n",
    "model = load_model(model_path,\n",
    "                   custom_objects={\"cross_entropy_balanced\": cross_entropy_balanced})\n",
    "print(\"\\n✓ Model loaded\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497ce843",
   "metadata": {},
   "source": [
    "## Step&nbsp;4 · Simple prediction on a validation sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532afe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_slices(gx_slice, fx_slice, fp_slice):\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    ax1.set_title(\"Input seismic\");       ax1.imshow(gx_slice.T, cmap='gray', vmin=-2, vmax=2)\n",
    "    ax2.set_title(\"Ground‑truth faults\"); ax2.imshow(fx_slice.T, cmap='gray', vmin=0, vmax=1)\n",
    "    ax3.set_title(\"Predicted faults\");    ax3.imshow(fp_slice.T, cmap='gray', vmin=0, vmax=1)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "sample_id = \"10\"\n",
    "gx_orig = np.load(os.path.join(validation_dir_new, \"seis\",  f\"{sample_id}.npy\"))\n",
    "fx_orig = np.load(os.path.join(validation_dir_new, \"fault\", f\"{sample_id}.npy\"))\n",
    "\n",
    "gx_proc = (gx_orig - gx_orig.mean()) / (gx_orig.std() + 1e-8)\n",
    "gx_tr   = np.transpose(gx_proc)   # match training orientation\n",
    "fx_tr   = np.transpose(fx_orig)\n",
    "\n",
    "inp = gx_tr.reshape(1, patch_n1, patch_n2, patch_n3, 1)\n",
    "fp_tr = model.predict(inp, verbose=0)[0, ..., 0]\n",
    "\n",
    "slice_idx = 64\n",
    "print(f\"Displaying slice {slice_idx} of validation cube {sample_id}\")\n",
    "plot_prediction_slices(gx_tr[slice_idx], fx_tr[slice_idx], fp_tr[slice_idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34b526a",
   "metadata": {},
   "source": [
    "## Step&nbsp;5 · Complex prediction on a large field image (tiling/patching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72feb470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gaussian_mask(overlap, n1, n2, n3):\n",
    "    \"\"\"3‑D Gaussian mask for smooth patch blending.\"\"\"\n",
    "    sig = overlap / 4.0\n",
    "    sig = 0.5 / (sig * sig)\n",
    "    ramp = np.exp(-((np.arange(overlap) - overlap + 1) ** 2) * sig).astype(np.single)\n",
    "\n",
    "    sc = np.ones((n1, n2, n3), dtype=np.single)\n",
    "    for i in range(overlap):\n",
    "        sc[i    , :, :] *= ramp[i]\n",
    "        sc[n1-1-i, :, :] *= ramp[i]\n",
    "        sc[:, i    , :] *= ramp[i]\n",
    "        sc[:, n2-1-i, :] *= ramp[i]\n",
    "        sc[:, :, i    ] *= ramp[i]\n",
    "        sc[:, :, n3-1-i] *= ramp[i]\n",
    "    return sc\n",
    "\n",
    "# 1️⃣  Load and transpose seismic volume\n",
    "f3_path = os.path.join(prediction_dir_f3d, \"gxl.dat\")\n",
    "d, i, x = 512, 384, 128   # depth, inline, xline\n",
    "gx = np.fromfile(f3_path, dtype=np.single).reshape(d, i, x)\n",
    "print(\"Loaded F3 volume:\", gx.shape, \"(depth, inline, xline)\")\n",
    "\n",
    "gx = gx.transpose(2, 1, 0)   # → (xline, inline, depth) == (nx, ny, nz)\n",
    "nx, ny, nz = gx.shape\n",
    "print(\"Transposed for model:\", gx.shape)\n",
    "\n",
    "# 2️⃣  Patch parameters\n",
    "overlap_size = 12\n",
    "stride = (patch_n1 - overlap_size,\n",
    "          patch_n2 - overlap_size,\n",
    "          patch_n3 - overlap_size)\n",
    "\n",
    "pad_x = (stride[0] - (nx - patch_n1) % stride[0]) % stride[0]\n",
    "pad_y = (stride[1] - (ny - patch_n2) % stride[1]) % stride[1]\n",
    "pad_z = (stride[2] - (nz - patch_n3) % stride[2]) % stride[2]\n",
    "\n",
    "gp = np.pad(gx, ((0, pad_x), (0, pad_y), (0, pad_z)), mode='constant')\n",
    "px, py, pz = gp.shape\n",
    "print(\"Padded to:\", gp.shape)\n",
    "\n",
    "gy = np.zeros_like(gp, dtype=np.single)\n",
    "mk = np.zeros_like(gp, dtype=np.single)\n",
    "mask = create_gaussian_mask(overlap_size, patch_n1, patch_n2, patch_n3)\n",
    "patch_buf = np.zeros((1, patch_n1, patch_n2, patch_n3, 1), dtype=np.single)\n",
    "\n",
    "print(\"\\nStarting tiled prediction …\")\n",
    "for x0 in tqdm(range(0, px - patch_n1 + 1, stride[0]), desc=\"x‑dim\"):\n",
    "    for y0 in range(0, py - patch_n2 + 1, stride[1]):\n",
    "        for z0 in range(0, pz - patch_n3 + 1, stride[2]):\n",
    "            patch = gp[x0:x0+patch_n1, y0:y0+patch_n2, z0:z0+patch_n3]\n",
    "            patch = (patch - patch.mean()) / (patch.std() + 1e-8)\n",
    "            patch_buf[0, ..., 0] = patch\n",
    "\n",
    "            pred = model.predict(patch_buf, verbose=0)[0, ..., 0]\n",
    "            gy[x0:x0+patch_n1, y0:y0+patch_n2, z0:z0+patch_n3] += pred * mask\n",
    "            mk[x0:x0+patch_n1, y0:y0+patch_n2, z0:z0+patch_n3] += mask\n",
    "\n",
    "# 4️⃣  Normalise & crop back to original size\n",
    "gy = np.divide(gy, mk, out=np.zeros_like(gy), where=mk != 0)\n",
    "fp_final = gy[:nx, :ny, :nz]          # still (xline, inline, depth)\n",
    "\n",
    "# Save in (xline, inline, depth) orientation\n",
    "pred_out_path = os.path.join(prediction_dir_f3d, \"fp_tensorflow.dat\")\n",
    "fp_final.astype(np.single).tofile(pred_out_path)\n",
    "print(\"\\n✓ Prediction finished — saved to\", pred_out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925c8a60",
   "metadata": {},
   "source": [
    "## Step&nbsp;6 · Visualise the field prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cf0aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload seismic in original orientation\n",
    "gx_orig = np.fromfile(os.path.join(prediction_dir_f3d, 'gxl.dat'), dtype=np.single).reshape(512, 384, 128)\n",
    "\n",
    "# Reload prediction and transpose back to (depth, inline, xline)\n",
    "fp_raw = np.fromfile(pred_out_path, dtype=np.single).reshape(128, 384, 512)\n",
    "fp_orig = fp_raw.transpose(2, 1, 0)\n",
    "\n",
    "print(\"Seismic shape    :\", gx_orig.shape)\n",
    "print(\"Prediction shape :\", fp_orig.shape)\n",
    "\n",
    "k_depth, k_inline, k_xline = 29, 29, 99\n",
    "\n",
    "# X‑line slice\n",
    "fig = plt.figure(figsize=(9,9))\n",
    "plt.suptitle(f'X‑line slice {k_xline}', y=0.82, fontsize=16)\n",
    "plt.subplot(1,2,1); plt.title(\"Seismic\");         plt.imshow(gx_orig[:, :, k_xline].T, aspect=1.5, cmap='gray')\n",
    "plt.subplot(1,2,2); plt.title(\"Fault prediction\"); plt.imshow(fp_orig[:, :, k_xline].T, aspect=1.5, cmap='gray', vmin=0.4, vmax=1.0)\n",
    "plt.show()\n",
    "\n",
    "# Inline slice\n",
    "fig = plt.figure(figsize=(9,9))\n",
    "plt.suptitle(f'Inline slice {k_inline}', y=0.82, fontsize=16)\n",
    "plt.subplot(1,2,1); plt.title(\"Seismic\");         plt.imshow(gx_orig[:, k_inline, :].T, aspect=1.5, cmap='gray')\n",
    "plt.subplot(1,2,2); plt.title(\"Fault prediction\"); plt.imshow(fp_orig[:, k_inline, :].T, aspect=1.5, cmap='gray', vmin=0.4, vmax=1.0)\n",
    "plt.show()\n",
    "\n",
    "# Depth slice\n",
    "fig = plt.figure(figsize=(9,9))\n",
    "plt.suptitle(f'Depth slice {k_depth}', y=0.82, fontsize=16)\n",
    "plt.subplot(1,2,1); plt.title(\"Seismic\");         plt.imshow(gx_orig[k_depth].T, cmap='gray')\n",
    "plt.subplot(1,2,2); plt.title(\"Fault prediction\"); plt.imshow(fp_orig[k_depth].T, cmap='gray', vmin=0.4, vmax=1.0)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
